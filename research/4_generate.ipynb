{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate Answer from Context\n",
    "\n",
    "This notebook tests the main generation node. It takes the user's question and the list of filtered, relevant documents and passes them to the LLM to synthesize a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.documents import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = \"llama3\"\n",
    "llm = Ollama(model=MODEL_NAME, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: dict, llm) -> dict:\n",
    "    \"\"\"\n",
    "    Generates an answer using the LLM with context.\n",
    "    This function is defined locally for experimentation.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING ANSWER WITH CONTEXT---\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are an assistant for question-answering tasks for oil and gas emissions compliance.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Be concise and provide the answer based only on the provided context.\n",
    "\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"question\", \"context\"])\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    context_str = \"\\n\\n\".join([d.page_content for d in documents])\n",
    "    generation = rag_chain.invoke({\"question\": question, \"context\": context_str})\n",
    "    return {\"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mock State ---\n",
    "# We will simulate the state after the 'grade_documents' node has run.\n",
    "question = \"What are the requirements for monitoring a flare's pilot flame?\"\n",
    "\n",
    "context_doc = Document(\n",
    "    page_content=\"The presence of a flare pilot flame must be continuously monitored using a thermocouple or any other equivalent device. If the pilot flame is extinguished, an alarm must be triggered, and corrective action must be taken within 5 minutes.\",\n",
    "    metadata={\"source\": \"sample_epa_regulation.txt\"}\n",
    ")\n",
    "\n",
    "state = {\n",
    "    \"question\": question,\n",
    "    \"documents\": [context_doc]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_answer(state, llm)\n",
    "\n",
    "print(\"--- Generated Answer ---\")\n",
    "print(result['generation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
