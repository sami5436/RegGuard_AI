{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generate Answer from Context\n",
    "\n",
    "This notebook tests the main generation node. It takes the user's question and the list of filtered, relevant documents and passes them to the LLM to synthesize a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.documents import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_64170/916712105.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=MODEL_NAME, temperature=0)\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = \"llama3\"\n",
    "llm = Ollama(model=MODEL_NAME, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: dict, llm) -> dict:\n",
    "    \"\"\"\n",
    "    Generates an answer using the LLM with context.\n",
    "    This function is defined locally for experimentation.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING ANSWER WITH CONTEXT---\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are an assistant for question-answering tasks for oil and gas emissions compliance.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Be concise and provide the answer based only on the provided context.\n",
    "\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"question\", \"context\"])\n",
    "    \n",
    "    # Takes a dictionary with a \"question\" and \"context\".\n",
    "    # formats it into a prompt.\n",
    "    # Sends the prompt to the language model to get a response.\n",
    "    # Parses the model's response to get a clean string as the final output.\n",
    "    \n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    context_str = \"\\n\\n\".join([d.page_content for d in documents])\n",
    "    generation = rag_chain.invoke({\"question\": question, \"context\": context_str})\n",
    "    return {\"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mock State ---\n",
    "# We will simulate the state after the 'grade_documents' node has run.\n",
    "question = \"What are the requirements for monitoring a flare's pilot flame?\"\n",
    "\n",
    "context_doc = Document(\n",
    "    page_content=\"The presence of a flare pilot flame must be continuously monitored using a thermocouple or any other equivalent device. If the pilot flame is extinguished, an alarm must be triggered, and corrective action must be taken within 5 minutes.\",\n",
    "    metadata={\"source\": \"sample_epa_regulation.txt\"}\n",
    ")\n",
    "\n",
    "state = {\n",
    "    \"question\": question,\n",
    "    \"documents\": [context_doc]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING ANSWER WITH CONTEXT---\n",
      "--- Generated Answer ---\n",
      "According to the provided context, the requirements for monitoring a flare's pilot flame are:\n",
      "\n",
      "* Continuous monitoring using a thermocouple or any other equivalent device.\n",
      "* An alarm must be triggered if the pilot flame is extinguished.\n",
      "* Corrective action must be taken within 5 minutes.\n"
     ]
    }
   ],
   "source": [
    "result = generate_answer(state, llm)\n",
    "\n",
    "print(\"--- Generated Answer ---\")\n",
    "print(result['generation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ig-reel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
