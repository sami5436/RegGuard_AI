{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Check Question Relevance\n",
    "\n",
    "This notebook tests the first node in our RAG pipeline. Its purpose is to make an initial LLM call to determine if the user's question is related to emissions. This prevents the system from performing an unnecessary document search for off-topic questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = \"llama3\"\n",
    "llm = Ollama(model=MODEL_NAME, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relevance(state: dict, llm) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if the question is relevant to the document context.\n",
    "    This function is defined locally for experimentation.\n",
    "    \"\"\"\n",
    "    print(\"---CHECKING QUESTION RELEVANCE---\")\n",
    "    question = state['question']\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are an expert in oil and gas regulations. Your task is to determine if a user's question is related to 'oil and gas emissions' or 'emissions' in general.\n",
    "    Answer with a simple 'yes' or 'no'.\n",
    "\n",
    "    User question: \"{question}\"\n",
    "\n",
    "    Is this question related to emissions? (yes/no):\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"question\"])\n",
    "    relevance_checker = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    relevance_response = relevance_checker.invoke({\"question\": question})\n",
    "    \n",
    "    if \"yes\" in relevance_response.lower():\n",
    "        print(\"---QUESTION IS RELEVANT---\")\n",
    "        return {\"is_relevant\": True}\n",
    "    else:\n",
    "        print(\"---QUESTION IS NOT RELEVANT---\")\n",
    "        return {\"is_relevant\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with a Relevant Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_question = \"What are the monitoring requirements for flare gas?\"\n",
    "state = {\"question\": relevant_question}\n",
    "\n",
    "result = check_relevance(state, llm)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with an Irrelevant Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_question = \"What is the capital of France?\"\n",
    "state = {\"question\": irrelevant_question}\n",
    "\n",
    "result = check_relevance(state, llm)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
